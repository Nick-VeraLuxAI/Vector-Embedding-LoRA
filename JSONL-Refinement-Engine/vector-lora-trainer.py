# === source ~/lora-env/bin/activate ===
# === sudo chown -R $USER:$USER /media/ndesantis/PS22001/LoRA/LoRA_FIles/lora-vector ===
# === torchrun --nproc_per_node=2 /home/ndesantis/Desktop/Vector-DB-LoRA/JSONL-Refinement-Engine/vector-lora-trainer.py ===
# === tensorboard --logdir="/media/ndesantis/PS22001/LoRA/LoRA_FIles/lora-vector/runs" ===


import os
import time
import torch
import torch.distributed as dist
import signal
import sys
import re
import torch.nn.functional as F
import matplotlib.pyplot as plt


from glob import glob
from datasets import load_dataset
from torch.utils.data import DataLoader, DistributedSampler
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
)
from peft import get_peft_model, LoraConfig
from torch.optim import AdamW
from torch.nn.parallel import DistributedDataParallel as DDP

# === ENVIRONMENT SETUP ===
os.environ["BITSANDBYTES_NOWELCOME"] = "1"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128,expandable_segments:True"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# === INIT DISTRIBUTED ===
dist.init_process_group("nccl")
local_rank = int(os.environ["LOCAL_RANK"])
torch.cuda.set_device(local_rank)
device = torch.device("cuda", local_rank)
rank = dist.get_rank()

# === PATHS ===
model_name = "mistralai/Mistral-7B-Instruct-v0.1"


data_path = "/home/ndesantis/Desktop/Vector-DB-LoRA/JSONL-Generation-Engine/content/vector-triplets-250K.jsonl"
output_dir = "/media/ndesantis/PS22001/LoRA/LoRA_FIles/lora-vector"


# === TOKENIZER & DATASET ===
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token
dataset = load_dataset("json", data_files=data_path, split="train")

# === Gives a Max Length ===
lengths = [(len(tokenizer(anchor + " " + positive)["input_ids"])) for anchor, positive in zip(dataset["anchor"], dataset["positive"])]
plt.hist(lengths, bins=50)
plt.title("Prompt+Response Token Lengths")
plt.xlabel("Number of tokens")
plt.ylabel("Count")
plt.show()
print("Max length:", max(lengths))
print("Average length:", sum(lengths) // len(lengths))
print("95th percentile:", sorted(lengths)[int(0.95 * len(lengths))])
print("99th percentile:", sorted(lengths)[int(0.99 * len(lengths))])



def tokenize(batch):
    prompts = [a + " " + p for a, p in zip(batch["anchor"], batch["positive"])]

    return tokenizer(prompts, truncation=True, padding="max_length", max_length=96)

tokenized = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)

# === DATALOADER ===
collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
sampler = DistributedSampler(tokenized)
dataloader = DataLoader(
    tokenized,
    batch_size=10,
    sampler=sampler,
    collate_fn=collator,
    num_workers=8,
    pin_memory=True,
)

# === VAL DATALOADER ===
val_sampler = DistributedSampler(tokenized, shuffle=True)
val_dataloader = DataLoader(
    tokenized,
    batch_size=8,
    sampler=val_sampler,
    collate_fn=collator,
    num_workers=4,
    pin_memory=True,
)


# === LOAD MODEL ===
if rank == 0:
    print("üîß Loading model...")

try:
    base_model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map={"": local_rank},
        use_safetensors=True,
        use_cache=False,
    )
except Exception as e:
    if rank == 0:
        print(f"‚ùå Failed to load model: {e}")
    dist.destroy_process_group()
    sys.exit(1)

base_model = base_model.to(device)

# === TARGET MODULES ===
if rank == 0:
    print("üîç Finding compatible target modules...")

target_modules = []
for name, module in base_model.named_modules():
    if isinstance(module, torch.nn.Linear) and module.in_features == module.out_features:
        if rank == 0:
            print(f"‚úÖ {name}: {module.in_features} ‚Üí {module.out_features}")
        target_modules.append(name.split(".")[-1])

target_modules = list(set(target_modules))
if rank == 0:
    print("üéØ Final target_modules:", target_modules)
    print(base_model.config.hidden_size)

base_model.config.use_cache = False

# === APPLY LORA ===
lora_config = LoraConfig(
    r=64,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    target_modules=target_modules,
    task_type="CAUSAL_LM"
)
model = get_peft_model(base_model, lora_config)
model.gradient_checkpointing_enable()
model.enable_input_require_grads()
model = model.to(device)
model.train()

# === OPTIMIZER ===
optimizer = AdamW(model.parameters(), lr=2e-4)

# === TensorBoad ===
from torch.utils.tensorboard import SummaryWriter

# üõ†Ô∏è Ensure the directory exists
os.makedirs(f"{output_dir}/runs", exist_ok=True)

writer = SummaryWriter(log_dir=f"{output_dir}/runs")


# === AUTOSCAN CHECKPOINT ===
start_epoch = 0
global_step = 0


def find_latest_valid_checkpoint(dirs):
    for ckpt_dir in sorted(dirs, key=lambda x: int(re.search(r"checkpoint-(\d+)", x).group(1)), reverse=True):
        full_state = os.path.join(ckpt_dir, "full_state.pt")
        try:
            torch.load(full_state, map_location="cpu", weights_only=False)
            return ckpt_dir  # Found the most recent valid one
        except Exception as e:
            if rank == 0:
                print(f"‚ùå Skipping {ckpt_dir}: {e}")
    return None

checkpoint_dirs = glob(os.path.join(output_dir, "checkpoint-*"))
latest_ckpt = find_latest_valid_checkpoint(checkpoint_dirs)
          

if latest_ckpt:
    checkpoint_path = os.path.join(latest_ckpt, "full_state.pt")
    if os.path.exists(checkpoint_path):
        if rank == 0:
            print(f"üîÑ Resuming from latest checkpoint: {latest_ckpt}")
        checkpoint = torch.load(checkpoint_path, map_location="cpu", weights_only=False)
        
        # ‚úÖ DEBUG PRINT
        if rank == 0:
            print("üß† Loaded checkpoint keys:", checkpoint.keys())

        # ‚úÖ Load states BEFORE wrapping in DDP
        if "model_state_dict" in checkpoint:
            model.load_state_dict(checkpoint["model_state_dict"], strict=False)
        if "optimizer_state_dict" in checkpoint:
            optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        if "epoch" in checkpoint:
            start_epoch = checkpoint["epoch"]
        if "global_step" in checkpoint:
            global_step = checkpoint["global_step"]

        if rank == 0:
            print(f"üîÅ Resumed from step {global_step}, epoch {start_epoch}")
    else:
        if rank == 0:
            print(f"‚ö†Ô∏è Checkpoint file not found: {checkpoint_path}")
else:
    if rank == 0:
        print("üÜï No checkpoints found ‚Äî starting from scratch")

# ‚úÖ Wrap in DDP only after checkpoint load (always do this once!)
model = DDP(model, device_ids=[local_rank])

import shutil

def save_checkpoint(model, tokenizer, optimizer, epoch, global_step, output_dir, rank, retry=False):
    ckpt_dir = f"{output_dir}/checkpoint-{global_step}"
    os.makedirs(ckpt_dir, exist_ok=True)

    if rank == 0:
        print(f"üíæ Saving checkpoint at step {global_step}...")

    try:
        # Save model and tokenizer safely
        model.module.save_pretrained(ckpt_dir, safe_serialization=True)
        tokenizer.save_pretrained(ckpt_dir)

        tmp_path = os.path.join(ckpt_dir, "full_state.tmp.pt")
        final_path = os.path.join(ckpt_dir, "full_state.pt")

        state_dict = {
            "epoch": epoch,
            "global_step": global_step,
            "optimizer_state_dict": optimizer.state_dict(),
            "model_state_dict": model.state_dict(),
        }

        with open(tmp_path, "wb") as f:
            torch.save(state_dict, f)
            f.flush()
            os.fsync(f.fileno())

        os.replace(tmp_path, final_path)

        # ‚úÖ Verify immediately
        try:
            torch.load(final_path, map_location="cpu", weights_only=False)
            if rank == 0:
                print(f"üì¶ Saved and verified checkpoint at step {global_step}")
        except Exception as verify_error:
            print(f"‚ùå Checkpoint verification failed: {verify_error}")
            shutil.rmtree(ckpt_dir)
            print(f"üßπ Corrupted checkpoint deleted: {ckpt_dir}")
            if not retry:
                print("üîÅ Retrying save once...")
                save_checkpoint(model, tokenizer, optimizer, epoch, global_step, output_dir, rank, retry=True)
            else:
                print("‚ùå Retry failed. Skipping save.")

    except Exception as e:
        print(f"‚ùå Failed to save checkpoint at step {global_step}: {e}")
        if os.path.exists(tmp_path):
            os.remove(tmp_path)




# === TRAINING LOOP ===
if rank == 0:
    print("üöÄ Starting training...")

epochs = 2
accum_steps = 5
optimizer.zero_grad()
t0 = time.time()

try:
    for epoch in range(start_epoch, epochs):
        sampler.set_epoch(epoch)
        for step, batch in enumerate(dataloader):
            if epoch == start_epoch and step < (global_step % len(dataloader)):
                continue

            for k in batch:
                batch[k] = batch[k].to(device, non_blocking=True)

            with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
                outputs = model(
                    input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"],
                    labels=batch["labels"]
                )
                loss = outputs.loss / accum_steps

            loss.backward()

            if (step + 1) % accum_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
                global_step += 1

                if rank == 0:
                    writer.add_scalar("train/loss", loss.item(), global_step)
                    print(f"Epoch {epoch+1} | Step {global_step} | Loss: {loss.item():.4f} | Time/step: {time.time() - t0:.2f}s")
                    t0 = time.time()

                    # === Validation ===
                    if global_step % 1000 == 0:
                        model.eval()
                        val_loss = 0
                        val_batches = 0
                        with torch.no_grad():
                            for val_batch in val_dataloader:
                                for k in val_batch:
                                    val_batch[k] = val_batch[k].to(device, non_blocking=True)
                                with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
                                    outputs = model(
                                        input_ids=val_batch["input_ids"],
                                        attention_mask=val_batch["attention_mask"],
                                        labels=val_batch["labels"]
                                    )
                                    val_loss += outputs.loss.item()
                                    val_batches += 1
                                    if val_batches >= 10:
                                        break
                        model.train()
                        avg_val_loss = val_loss / val_batches

                        elapsed = time.time() - t0
                        tokens_this_step = batch["input_ids"].numel()
                        tokens_per_sec = tokens_this_step / elapsed
                        gpu_cost_per_hour = 0.15
                        cost_per_step = (elapsed / 3600) * gpu_cost_per_hour

                        writer.add_scalar("val/loss", avg_val_loss, global_step)
                        writer.add_scalar("perf/tokens_per_sec", tokens_per_sec, global_step)
                        writer.add_scalar("perf/approx_cost_usd", cost_per_step, global_step)

                        print(f"üß™ Validation Loss @ Step {global_step}: {avg_val_loss:.4f}")
                        print(f"‚öôÔ∏è  {tokens_per_sec:.2f} tokens/sec | üí∏ ${cost_per_step:.4f} / step")

                    # === Checkpointing ===
                    if global_step % 250 == 0:
                        ckpt_dir = f"{output_dir}/checkpoint-{global_step}"
                        os.makedirs(ckpt_dir, exist_ok=True)

                        max_retries = 3
                        for attempt in range(1, max_retries + 1):
                            try:
                                model.module.save_pretrained(ckpt_dir, safe_serialization=True)
                                tokenizer.save_pretrained(ckpt_dir)

                                tmp_path = os.path.join(ckpt_dir, "full_state.tmp.pt")
                                final_path = os.path.join(ckpt_dir, "full_state.pt")

                                state_dict = {
                                    "epoch": epoch,
                                    "global_step": global_step,
                                    "optimizer_state_dict": optimizer.state_dict(),
                                    "model_state_dict": model.state_dict(),
                                }

                                with open(tmp_path, "wb") as f:
                                    torch.save(state_dict, f)
                                    f.flush()
                                    os.fsync(f.fileno())

                                os.replace(tmp_path, final_path)

                                # üîç Verify checkpoint
                                try:
                                    torch.load(final_path, map_location="cpu", weights_only=False)
                                    print(f"üì¶ Saved and verified checkpoint at step {global_step}")
                                    break
                                except Exception as e:
                                    print(f"‚ùå Checkpoint verification failed: {e}")
                                    import shutil
                                    shutil.rmtree(ckpt_dir, ignore_errors=True)
                                    print(f"üßπ Deleted corrupted checkpoint: {ckpt_dir}")
                                    if attempt == max_retries:
                                        print("‚ùå Max retries reached. Skipping this checkpoint.")
                                    else:
                                        print(f"üîÅ Retrying save... ({attempt}/{max_retries})")
                                        time.sleep(3)

                            except Exception as e:
                                print(f"‚ùå Checkpoint save failed: {e}")
                                if attempt == max_retries:
                                    print("‚ùå Max retries reached. Giving up on save.")
                                else:
                                    print(f"üîÅ Retrying save... ({attempt}/{max_retries})")
                                    time.sleep(3)

except KeyboardInterrupt:
    if rank == 0:
        print("üõë Interrupted manually. Performing final save...")

finally:
    if rank == 0:
        print("üíæ Final save and shutdown...")
        model.module.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)

    dist.destroy_process_group()
    torch.cuda.empty_cache()
